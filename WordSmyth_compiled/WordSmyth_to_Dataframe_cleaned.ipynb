{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code in sections to:\n",
    "1. scrape WordSmyth as a set of html webpages from the internet\n",
    "2. load and parse text from the downloaded set of webpages into a plain text file\n",
    "3. organise text file into a dictionary dataframe\n",
    "4. merge or otherwise modify glosses in dictionary dataframe to take into account cross-references among glosses\n",
    "5. tally the number of senses and meaning in the dataframe\n",
    "\n",
    "NOTES: \n",
    "1. assumes file structure where script and scraped WordSmyth html files are in the same folder, with subfolder for html files\n",
    "3. interim dataframe can be saved at various steps to allow for manual checking and modification before next step -- checks are recommended for first-pass due to idiosyncracies in a small percentage of words/ glosses\n",
    "4. there are sections which may do more than is needed -- can be skipped or substituted with modified input at next step\n",
    "5. by default has a filter by existence of covariates (SUBTLwf, nLet, nSyll, nPhon, coltNOrth, coltNPhon, old20, pld20, posBigram, isHomograph, isHomophone) taken from http://blairarmstrong.net/tools/Union_Subtl_CMUPron/index.html which requires a separate \"fullCov.csv\" file which is included in the same folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set filepaths and parameters in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter setting for: \n",
    "\n",
    "##1. subfolder name for data output/ html downloads\n",
    "outFolderName = \"/Oct2019\" # html files originally scraped (and downloaded) in Oct 2019\n",
    "\n",
    "##2. whether to first conduct scrape from internet (onlineScrape = True) to download html files into zipped folder\n",
    "##   or parse directly from downloaded html files in zipped folder (onlineScrape = False)\n",
    "### online: set WordSmyth page range (by default from 1 to 61495, which covers all valid pages as of Oct 2019)\n",
    "### offline: set folder to find downloaded html files -- by default same as outFolderName; \n",
    "###          set identifying tag for paresing files (from scrape to dataframe) -- by default day of parse\n",
    "from datetime import date\n",
    "\n",
    "onlineScrape = False\n",
    "\n",
    "if onlineScrape == True:\n",
    "    minpage = 1\n",
    "    maxpage = 61495\n",
    "else:\n",
    "    htmlFolderName = outFolderName\n",
    "    parsetag = str(date.today())\n",
    "\n",
    "##3. whether to use full word list from the dictionary or only a partial/filtered list\n",
    "### set function to filter, currently by column in dataframe (NOTE: dictionary merged with filter list in dataframe if toFilter)\n",
    "\n",
    "### currently filtered by presence of a subset of covariate measures; other filtering list in .csv format can be substituted\n",
    "\n",
    "toFilter = False\n",
    "\n",
    "filterFileName = \"/fullCov.csv\"\n",
    "\n",
    "if toFilter == True:\n",
    "    filetag = \"filtered\"\n",
    "else:\n",
    "    filetag = \"allwords\"\n",
    "\n",
    "    \n",
    "def filterFnc(parseDF, filterDFName): #parseDF = parsed and flagged dictionary dataframe, filterDF = filter file name\n",
    "    filterDF = pd.read_csv(folderpath + filterDFName)\n",
    "    \n",
    "    parseDF = parseDF.merge(filterDF, 'left', on = 'word')\n",
    "    \n",
    "    parseDF = (parseDF[parseDF['old20'].notnull()]).copy(deep=True).reset_index(drop=True)\n",
    "    \n",
    "    return parseDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get file path for current folder (assumes wordsmyth files are saved in same folder as script in a folder named)\n",
    "\n",
    "import os\n",
    "folderpath = os.path.abspath('')\n",
    "\n",
    "#depreciated:\n",
    "#folderpath = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "print(folderpath)\n",
    "\n",
    "if not os.path.exists(folderpath + outFolderName):\n",
    "    os.makedirs(folderpath + outFolderName)\n",
    "\n",
    "if onlineScrape == True:  \n",
    "    if not os.path.exists(folderpath + outFolderName + \"/wordsmyth_html\"):\n",
    "        os.makedirs(folderpath + outFolderName + \"/wordsmyth_html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordSmyth Scraper (and download) saves dictionary text (word, part-of-speech, definition) to a .txt file\n",
    "\n",
    "import re\n",
    "import multiprocessing\n",
    "import collections\n",
    "import csv\n",
    "import os\n",
    "import ssl\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "#import urllib2 #python 2\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import wget\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "#from importlib import reload\n",
    "from datetime import date\n",
    "from itertools import compress\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#from nltk.stemmer import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "#from multiprocessing import Pool\n",
    "if sys.version[0] == '2': #(python 3 already uses utf-8 by default)\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "#function to parse html page (same regardless of whether online or offline)\n",
    "##takes in html page and log file to write to; does not return variable\n",
    "##NOTE: some formatting has changed in html post original scrape/html files from Oct 2019\n",
    "\n",
    "\n",
    "##########get pages from website##########\n",
    "\n",
    "if onlineScrape == True:\n",
    "    logf = open(folderpath + outFolderName + \"/ScrapeExceptions\" + parsetag + \".txt\", \"w+\")\n",
    "\n",
    "    for i in range(minpage,maxpage):\n",
    "\n",
    "        url = \"https://www.wordsmyth.net/?level=3&ent=&rid=\" + str(i) + \"&f_div=lc_div_alpha\"\n",
    "    \n",
    "        while True:\n",
    "            try:\n",
    "                #page = requests.get(url, verify=False, timeout=30) #python3\n",
    "                page = urllib.request.urlopen(url, timeout=30)\n",
    "                #response = urllib2.urlopen(url, timeout = 30) #python2\n",
    "                break\n",
    "            #except urllib2.HTTPError, detail: #python2\n",
    "            except urllib.error.HTTPError as detail:\n",
    "                if detail.errno == 500:\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                else:\n",
    "                    logf.write(str(i)+' error\\n')\n",
    "                    raise MyException(\"Word ID \" + str(i) + \"failed\")\n",
    "        \n",
    "        #python3:\n",
    "        fid = open(folderpath + outFolderName + \"/wordsmyth_html/WS\" + str(i) + \".html\", \"wb\")\n",
    "        shutil.copyfileobj(page, fid)\n",
    "        fid.close()\n",
    "        \n",
    "        #python2:\n",
    "        #page = response.read() \n",
    "        #with open(folderpath + outFolderName + '/wordsmyth_html/WS'+str(i) +'.html', 'w') as fid:\n",
    "            #fid.write(page)\n",
    "            \n",
    "        #soup = BeautifulSoup(page, 'html.parser')\n",
    "        \n",
    "    logf.close()\n",
    "\n",
    "    #zip from uncompressed folder\n",
    "    with zipfile.ZipFile(folderpath + outFolderName + '/wordsmyth_html.zip', 'w') as zip_object:\n",
    "        for folder_name, sub_folders, file_names in os.walk(folderpath + outFolderName + '/wordsmyth_html'):\n",
    "            for filename in file_names:\n",
    "                file_path = os.path.join(folder_name, filename)\n",
    "                zip_object.write(file_path, os.path.basename(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open a file for main scrape and a file to log exceptions\n",
    "f = open(folderpath + outFolderName + \"/WordSmythScrape\" + parsetag + \".txt\",\"w+\", encoding=\"utf-8\", errors=\"surrogateescape\")\n",
    "logf = open(folderpath + outFolderName + \"/ScrapeExceptions\" + parsetag + \".txt\", \"a+\", encoding=\"utf-8\", errors=\"surrogateescape\")\n",
    "\n",
    "##########get pages from saved html##########\n",
    "\n",
    "#extraction from zip folder\n",
    "with zipfile.ZipFile(folderpath + outFolderName + \"/wordsmyth_html.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(folderpath + outFolderName)\n",
    "\n",
    "WSpages = [file for file in os.listdir(folderpath + outFolderName + \"/wordsmyth_html/\") if file.endswith('.html')]\n",
    "for fname in WSpages:\n",
    "    print(fname)\n",
    "    soup = BeautifulSoup(open(folderpath + outFolderName + \"/wordsmyth_html/\" + fname), \"html.parser\")\n",
    "        \n",
    "############################parsing pages\n",
    "\n",
    "    #break if id has no content\n",
    "    if len(soup.find_all('div', {'class':'notfound'})) != 0:\n",
    "        #logf.write(str(i)+' missing \\n') #from web\n",
    "        logf.write(fname+' missing \\n') #from file\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    word = soup.find_all('h3', {'class':\"headword\"})[-1].get_text() #NOTE: 'h1' in scrapes after Oct 2019\n",
    "\n",
    "    inflections = []\n",
    "    alltd = soup.find_all('td', {\"class\":\"title\"})\n",
    "    for td in alltd:\n",
    "        if len(td.find_all('a', {'title':\"Click to learn more about parts of speech\"})) > 0 :\n",
    "            #inflections.append('pos=' + td.next_sibling.next_sibling.getText().encode('utf-8')) #python2\n",
    "            inflections.append('pos=' + td.next_sibling.next_sibling.getText())\n",
    "        if len(td.find_all('a', {\"title\":\"Click to learn more about inflections\"})) > 0 :\n",
    "            #inflections.append(td.next_sibling.next_sibling.getText().encode('utf-8')) #python2\n",
    "            inflections.append(td.next_sibling.next_sibling.getText())\n",
    "    \n",
    "    inflections_paired=[]\n",
    "    for i in range(len(inflections)-1):\n",
    "        if inflections[i][0:4] == 'pos=' and inflections[i+1][0:4] != 'pos=':\n",
    "            inflections_paired.append((inflections[i][4:], inflections[i+1]))\n",
    "\n",
    "    #pos = soup.find_all('tr', class_=\"postitle\")\n",
    "    defn = soup.find_all('tr', {'class':['postitle','definition']})\n",
    "    \n",
    "    g=0\n",
    "    \n",
    "    f.write(\"#\" + word + '\\n')\n",
    "    \n",
    "    for d in defn:\n",
    "        box = d.find_all('dl')\n",
    "        if type(box)!=type(None):\n",
    "            for b in box:\n",
    "                b.extract()\n",
    "        if d['class'][0]=='postitle':\n",
    "            pos = d.get_text().split('\\n')[2]\n",
    "            f.write('@'+pos+'\\n')\n",
    "            f.write('infn:' + str([i[1] for i in inflections_paired if i[0] == pos]) + '\\n')\n",
    "        elif d['class'][0]=='definition':\n",
    "            g=g+1\n",
    "\n",
    "            f.write(\"defn\"+str(g)+\":\")\n",
    "            if type(d.find_all(\"a\"))!=type(None):\n",
    "                for l in d.find_all(\"a\"):\n",
    "                    l.extract()\n",
    "            if type(d.find_all(\"em\"))!=type(None):\n",
    "                for e in d.find_all(\"em\"):\n",
    "                    e.extract()\n",
    "            f.write(d.get_text().strip()+'\\n')\n",
    "        \n",
    "logf.close()\n",
    "f.close()\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get basic dictionary (as is) as dataframe from text\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def readfile(fname):\n",
    "    #import codecs #python 2\n",
    "    #with codecs.open(fname, encoding='utf-8') as f:\n",
    "\n",
    "    with open(fname, \"r\", encoding='utf-8') as f:\n",
    "    \n",
    "        dic = f.readlines()\n",
    "\n",
    "    return dic\n",
    "\n",
    "#how text file from scrape (above) is written: lemmas start with #, pos start with @, \n",
    "##inflections start with infn:, definitions start with defn*:\n",
    "#to populate dataframe, when # is encountered, lemma is set till next #; when @ is encountered, pos is set till next @, \n",
    "##one definition per line, inheriting current lemma and pos\n",
    "#once dataframe is populated, separate number in lemma from string: \n",
    "##if there are repeated string entries in dictionary, leave it\n",
    "##if there are no repeated string entries in dictionary, delete entry from dataframe (not proper word)\n",
    "\n",
    "\n",
    "#THE FOLLOWING CAN BE SUBSTITUTED FOR OTHER SCRAPED FILES\n",
    "#text = readfile('C:\\\\Users\\\\modi1\\\\Dropbox\\\\PhD\\\\vectors code\\\\dictionary\\\\Oct2019\\\\WordSmythScrape2019-10-29.txt')\n",
    "text = readfile(folderpath + outFolderName + \"/WordSmythScrape\" + parsetag + \".txt\")\n",
    "\n",
    "lemma = \"*\"\n",
    "word = \"*\"\n",
    "pos = \"^\"\n",
    "infln = []\n",
    "defn = []\n",
    "meaningNo = 0\n",
    "glossNo = 0\n",
    "meanings = [['wordOG', 'word', 'meaningNo', 'infln', 'pos', 'gloss', 'glossNo']]\n",
    "#meanings = pd.DataFrame(columns=['word', 'pos', 'gloss'])\n",
    "#l=0\n",
    "#progress = 0\n",
    "for line in text:\n",
    "    #progress = progress + 1\n",
    "    #print (str(progress/(sum(1 for line in text))) + '%')\n",
    "    if line.startswith(\"#\"):\n",
    "        pos = \"^\"\n",
    "        defn = []\n",
    "        infln = \"\"\n",
    "        word = line[line.index(\"#\")+1:].strip()\n",
    "        lemma = word[:word.rfind(re.findall('[^0-9]', word)[-1])+1]\n",
    "        wordN = word[word.rfind(re.findall('[^0-9]', word)[-1])+1:]\n",
    "    elif line.startswith(\"infn\"):\n",
    "        infln = line[line.index(\"[\")+1:line.index(\"]\")-1]\n",
    "    elif line.startswith(\"@\"):\n",
    "        pos = line[line.index(\"@\")+1:]\n",
    "    elif line.startswith(\"defn\"):\n",
    "        defn = line[line.index(\":\")+1:]\n",
    "        defnN = line[4:line.index(\":\")]\n",
    "        #meanings.at[l, 'word'] = lemma.strip()\n",
    "        #meanings.at[l, 'pos'] = pos.strip()\n",
    "        #meanings.at[l, 'gloss'] = defn.strip()\n",
    "        #l=l+1\n",
    "        meanings.append([word, lemma, wordN, infln.strip('u\\''), pos.strip(),defn.strip(), defnN])\n",
    "\n",
    "        \n",
    "dic = pd.DataFrame(meanings).T.set_index(0).T\n",
    "#output basic dictionary as csv\n",
    "dic.to_csv(folderpath + outFolderName + '/dictionary' + parsetag + '.csv', index = False, encoding='utf-8')\n",
    "#pickle file for python use (reading csv results in evaluation of protected words like \"null\")\n",
    "dic.to_pickle(folderpath + outFolderName + '/dictionary' + parsetag + '.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take basic dictionary dataframe and flag for things to exclude/ merge/ check\n",
    "##NEW: search for words with alternative spellings \n",
    "## -- check if they all lead back to the same glosses anyway or have different entries\n",
    "##remove lemmas with more than one word (search for space)\n",
    "##remove glosses where pos are abbreviations\n",
    "##check if word is inflected form -- not counted if word same as its own inflections\n",
    "\n",
    "#recommendation for manual check (some are not uniform indicators; remove anything that is not proper word):\n",
    "## definitions that contain the string \"abbr.\" \n",
    "## referents which are not found for manual check (printed as cell output)\n",
    "\n",
    "\n",
    "#THE FOLLOWING CAN BE SUBSTITUTED FOR OTHER PICKLED DICTIONARY FILES\n",
    "#dic = pd.read_pickle('C:\\\\Users\\\\modi1\\\\Dropbox\\\\PhD\\\\vectors code\\\\dictionary\\\\Oct2019\\\\dictionary.pkl')\n",
    "dic = pd.read_pickle(folderpath + outFolderName + '/dictionary' + parsetag + '.pkl')\n",
    "\n",
    "dic_flagged = dic.copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "dic_flagged[\"isPhrase\"] = 0\n",
    "\n",
    "dic_flagged[\"isAbbr\"] = 0\n",
    "\n",
    "dic_flagged[\"isInfln\"] = 0\n",
    "\n",
    "#for each gloss that is phrase/abbreviation, get word\n",
    "\n",
    "phraseList=[]\n",
    "abbrList=[]\n",
    "for i in dic_flagged.index:\n",
    "    if 'phrase' in dic_flagged.at[i,'pos']:\n",
    "        phraseList.append(dic_flagged.at[i,'word'])\n",
    "    elif 'abbr' in dic_flagged.at[i,'pos']:\n",
    "        abbrList.append(dic_flagged.at[i,'word'])\n",
    "\n",
    "#for each word that has pos with phrase or abbreviation, flag in dataframe        \n",
    "dic_flagged.loc[dic_flagged['word'].isin(phraseList),'isPhrase'] = 1\n",
    "dic_flagged.loc[dic_flagged['word'].isin(abbrList),'isAbbr'] = 1\n",
    "\n",
    "\n",
    "#get list of inflections        \n",
    "#inflnList=sum(([i.split(\", \") for i in pd.unique(dic['infln'])]),[])\n",
    "inflnList=[i for i in pd.unique(dic['infln'])]\n",
    "\n",
    "#for each word that is an inflection of another word, flag in dataframe\n",
    "##note that some words are exactly the same as their corresponding inflection; unflag these\n",
    "dic_flagged.loc[dic_flagged['word'].isin(inflnList),'isInfln'] = 1\n",
    "\n",
    "for i in dic_flagged.index:\n",
    "    if not pd.isna(dic_flagged.at[i, 'infln']):\n",
    "        if dic_flagged.at[i, 'word'] in dic_flagged.at[i, 'infln'].split(', '):\n",
    "            dic_flagged.at[i, 'isInfln'] = 0 \n",
    "        \n",
    "#flag glosses that refers to other glosses, what gloss are they refering to? ALSO CHECK THIS MANUALLY\n",
    "possibleRef=('variant of ', 'pl. of', 'plural of', 'past participle of', 'past tense of', 'present participle of')\n",
    "for i in dic_flagged.index:\n",
    "    if dic_flagged.at[i, 'isInfln'] == 1: \n",
    "        dic_flagged.at[i, 'hasRef'] = 1\n",
    "        dic_flagged.at[i, 'ref'] = list(dic_flagged[dic_flagged['infln'].str.contains(', ' + dic_flagged.at[i, 'word'] + '$| ' + dic_flagged.at[i, 'word'] + '\\,|^' + dic_flagged.at[i, 'word'] + '$|^' + dic_flagged.at[i, 'word'] + '\\,')==True][['word', 'pos']].drop_duplicates().itertuples(index=False, name=None))\n",
    "    if dic_flagged.at[i, 'gloss'].startswith('see '): \n",
    "        dic_flagged.at[i, 'hasRef'] = 1\n",
    "        dic_flagged.at[i, 'ref'] = dic_flagged.at[i, 'gloss'][dic_flagged.at[i, 'gloss'].index('see ')+4:-1]\n",
    "    if any(s in dic_flagged.at[i, 'gloss'] for s in possibleRef):\n",
    "        dic_flagged.at[i, 'hasRef'] = 1\n",
    "        dic_flagged.at[i, 'ref'] = dic_flagged.at[i, 'gloss'][dic_flagged.at[i, 'gloss'].rindex(' of ')+4:-1]\n",
    "\n",
    "dic_flagged['hasRef']=dic_flagged['hasRef'].fillna(value = 0)\n",
    "dic_flagged['meaningNo']=dic_flagged['meaningNo'].fillna(value = 1)\n",
    "\n",
    "\n",
    "#merge with filtering file (default is a dataframe of words with full set of covariates + wordnet senses)\n",
    "if toFilter == True:\n",
    "    dic_flagged = filterFnc(dic_flagged, filterFileName)\n",
    "    \n",
    "#    #CAN BE SUBSTITUTED\n",
    "#    #fullCov = pd.read_csv(\"C:\\\\Users\\\\modi1\\\\Dropbox\\\\PhD\\\\vectors code\\\\dictionary\\\\fullCov.csv\") \n",
    "#    filterDF = pd.read_csv(folderpath + outFolderName + filterFileName)\n",
    "#    \n",
    "#    dic_flagged = dic_flagged.merge(filterDF, 'left', on='word') \n",
    "#    #ADD ANY FILTERING FUNCTION HERE\n",
    "#    #dic_flagged = (dic_flagged[dic_flagged['old20'].notnull()]).copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#output dictionary dataframe flagged for special cases and merged covariates\n",
    "dic_flagged.to_csv(folderpath + outFolderName + '/dic_flagged_' + filetag + parsetag + '.csv', index = False, encoding='utf-8')\n",
    "dic_flagged.to_pickle(folderpath + outFolderName + '/dic_flagged_' + filetag + parsetag + '.pkl') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended to do manual checking of the dic_flagged output here for accuracy of filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Append/ replace any gloss with external referents as flagged above\n",
    "\n",
    "import regex\n",
    "import re\n",
    "import numpy as np\n",
    "import regex\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#import MANUALLY CHECKED LIST (take only rows with covariates)\n",
    "#dic_flagged = pd.read_csv(\"C:\\\\Users\\\\modi1\\\\Dropbox\\\\PhD\\\\vectors code\\\\dictionary\\\\Oct2019\\\\dicflagged_trimmed.csv\") \n",
    "\n",
    "\n",
    "#OR use unfiltered variable created from previous section\n",
    "#dic_flagged.merge(fullCov) #if using unfiltered variable created from previous section\n",
    "dic_flagged = pd.read_pickle(folderpath + outFolderName + '/dic_flagged_' + filetag + parsetag + '.pkl')\n",
    "\n",
    "#intiate new merged_dictionary\n",
    "merged_dic = pd.DataFrame(columns=list(dic_flagged.columns) + ['mergedSense', 'mergedGlossNo'])\n",
    "\n",
    "#get unique word list of all dictionary words (not stripped for meaningNo)\n",
    "#ANY FURTHER FILTERING FUNCTION TO BE ADDED HERE\n",
    "allwords = pd.unique(dic_flagged['wordOG'])\n",
    "\n",
    "possibleRef=('variant of ', 'pl. of', 'plural of', 'past participle of', 'past tense of', 'present participle of')\n",
    "\n",
    "#print to file cases where referent marked as existent but is not found -- needs manual check\n",
    "reflog = open(folderpath + outFolderName + \"/checkreferent_\" + filetag + parsetag + \".txt\",\"w+\", encoding=\"utf-8\")\n",
    "\n",
    "dropsense = False\n",
    "#for each word, get all senses as separate df\n",
    "for w in allwords:\n",
    "    #sensedf = []\n",
    "    sensedf = (dic_flagged.loc[dic_flagged['wordOG'] == w]).copy(deep=True).reset_index(drop=True)\n",
    "    sensedf2 = sensedf.copy(deep=True) #to refer back to for creating sensedf_add (in case sensedf has no more rows after edit)\n",
    "##check if word has referent: if no, mark appendedSense as 0 for all rows and go to next word;\n",
    "    if sum(sensedf['hasRef']) == 0:\n",
    "        sensedf['mergedSense'] = 0\n",
    "        sensedf['mergedGlossNo'] = sensedf['glossNo']\n",
    "        #merged_dic = merged_dic.append(sensedf)\n",
    "###if word has referent: go through line by line to see what kind of reference it is: \n",
    "####ignore null reference\n",
    "####extract all tuples as tuples\n",
    "####check what is left, if any for need to append pos\n",
    "    else:\n",
    "        reflist = []\n",
    "\n",
    "        for i in sensedf.index.values:\n",
    "            dropsense = False\n",
    "            if type(sensedf.at[i,'ref']) == float:\n",
    "                continue\n",
    "            #alltuples = [tuple(t.split(', ')) for t in re.findall('\\((.*?)\\)', str(sensedf.at[i,'ref']))]\n",
    "#######all tuples of word and pos. Note that outermost parentheses taken to be delimiter as there are nested parentheses\n",
    "            alltuples = [tuple(str(sensedf.at[i,'ref'])[t.start()+1:t.end()-1].split(', ')) for t in regex.finditer(r'\\(([^()]|(?R))*\\)', str(sensedf.at[i,'ref']))]\n",
    "            reflist.extend(alltuples)\n",
    "#######everything else\n",
    "            sense_i = re.sub('\\([^>]+\\)', '', str(sensedf.at[i,'ref']).strip('[]'))\n",
    "            \n",
    "            if not any(i.isalnum() for i in sense_i):\n",
    "                continue\n",
    "####if gloss refers to referent directly delete original gloss\n",
    "####Note: for sense inflections, take only verb senses; for plurals, take only noun senses \n",
    "            #if any(s in sensedf.at[i, 'gloss'] for s in possibleRef) or sensedf.at[i, 'gloss'].startswith('see '):\n",
    "            #    reflist.append(sensedf.loc[i, 'ref'])\n",
    "            if any(s in sensedf.at[i,'gloss'] for s in ('pl. of', 'plural of')):\n",
    "                dropsense = True\n",
    "                #nounsenses = (sensedf.loc[i, 'ref']).split(', ')\n",
    "                nounsenses = [n for n in sense_i.split(', ') if any(let.isalnum() for let in n)]\n",
    "                #reflist.extend(tuple(zip(nounsenses, ['(?<!pro)noun'])))\n",
    "                reflist.extend(map(lambda s: (s, '(?<!pro)noun'), nounsenses))\n",
    "                \n",
    "            if any(s in sensedf.at[i,'gloss'] for s in ('past tense of', 'past participle of', 'present participle of')):\n",
    "                dropsense = True\n",
    "                #verbsenses = (sensedf.loc[i, 'ref']).split(', ')\n",
    "                verbsenses = [v for v in sense_i.split(', ') if any(let.isalnum() for let in v)]\n",
    "                #reflist.extend(tuple(zip(verbsenses, ['(?<!ad)verb'])))\n",
    "                reflist.extend(map(lambda s: (s, '(?<!ad)verb'), verbsenses))\n",
    "                \n",
    "            if ('variant of' in sensedf.at[i,'gloss']) or (sensedf.at[i, 'gloss'].startswith('see ')):\n",
    "                dropsense = True\n",
    "                #reflist.extend((sensedf.loc[i, 'ref']).split(', '))\n",
    "                reflist.extend([lem for lem in sense_i.split(', ') if any(let.isalnum() for let in lem)])               \n",
    "                               \n",
    "            if dropsense == False:\n",
    "            #if (all(s not in sensedf.at[i,'gloss'] for s in possibleRef) and not ('variant of' in sensedf.at[i,'gloss']) and not (sensedf.at[i, 'gloss'].startswith('see ')):\n",
    "                reflist.extend([lem for lem in sense_i.split(', ') if any(let.isalnum() for let in lem)])\n",
    "                           \n",
    "            if dropsense == True:\n",
    "                sensedf.drop([i], inplace = True)\n",
    "\n",
    "        sensedf['mergedSense'] = 0\n",
    "###append UNIQUE referent senses to word (flag as appendedSense) \n",
    "###[Note that some referents are in the form of list of tuples]\n",
    "###keep count of how many new senses there are to append\n",
    "        #for r in sensedf.ref:\n",
    "        #    if type(r) != float and r not in reflist:\n",
    "        #        reflist.extend(r)\n",
    "        #print(reflist)\n",
    "        for r in range(len(reflist)):\n",
    "            if type(reflist[r]) == str:\n",
    "                if not dic_flagged['wordOG'].eq(reflist[r]).any():\n",
    "                    reflog.write(\"referent not found: word = \" + w + \", referent = \" + reflist[r] + \"\\n\")\n",
    "            elif type(reflist[r]) == tuple:\n",
    "                if reflist[r][0][-1].isdigit():\n",
    "                    if not dic_flagged['wordOG'].eq(reflist[r][0]).any():    \n",
    "                        reflog.write(\"referent not found: word = \" + w + \", referent = \" + reflist[r][0] + \"\\n\")\n",
    "                else:\n",
    "                    if not dic_flagged['word'].eq(reflist[r][0]).any():    \n",
    "                        reflog.write(\"referent not found: word = \" + w + \", referent = \" + reflist[r][0] + \"\\n\")\n",
    "                        \n",
    "            if (type(reflist[r]) == tuple and len(reflist[r]) == 2):\n",
    "                if reflist[r][1] == 'transitive verb':\n",
    "                    refpos = '(?<!in)transitive verb'\n",
    "                else:\n",
    "                    refpos = reflist[r][1]\n",
    "\n",
    "            if r == 0:\n",
    "                if type(reflist[r]) == str:\n",
    "                    refsenses = (dic_flagged.loc[dic_flagged['wordOG'] == reflist[r]]).copy(deep=True).reset_index(drop=True)\n",
    "                elif type(reflist[r]) == tuple:\n",
    "                    if reflist[r][0][-1].isdigit():\n",
    "                        refsenses = (dic_flagged.loc[(dic_flagged['wordOG'] == reflist[r][0]) & (dic_flagged['pos'].str.contains(refpos))]).copy(deep=True).reset_index(drop=True)\n",
    "                    else:\n",
    "                        refsenses = (dic_flagged.loc[(dic_flagged['word'] == reflist[r][0]) & (dic_flagged['pos'].str.contains(refpos))]).copy(deep=True).reset_index(drop=True)\n",
    "            else:\n",
    "                if type(reflist[r]) == str:\n",
    "                    pd.concat([refsenses, (dic_flagged.loc[dic_flagged['wordOG'] == reflist[r], ('pos', 'gloss')])], axis=0)\n",
    "                    #refsenses.append((dic_flagged.loc[dic_flagged['wordOG'] == reflist[r], ('pos', 'gloss')]).copy(deep=True).reset_index(drop=True))\n",
    "                elif type(reflist[r]) == tuple: \n",
    "                    if reflist[r][0][-1].isdigit():\n",
    "                        pd.concat([refsenses, (dic_flagged.loc[dic_flagged['wordOG'] == reflist[r][0], ('pos', 'gloss')])], axis=0)\n",
    "                        #refsenses.append((dic_flagged.loc[(dic_flagged['wordOG'] == reflist[r][0]) & (dic_flagged['pos'].str.contains(refpos)), ('pos', 'gloss')]).copy(deep=True).reset_index(drop=True))\n",
    "                    else:\n",
    "                        pd.concat([refsenses, (dic_flagged.loc[dic_flagged['word'] == reflist[r][0], ('pos', 'gloss')])], axis=0)\n",
    "                        #refsenses.append((dic_flagged.loc[(dic_flagged['word'] == reflist[r][0]) & (dic_flagged['pos'].str.contains(refpos)), ('pos', 'gloss')]).copy(deep=True).reset_index(drop=True))\n",
    "\n",
    "        sensedf_add = pd.DataFrame(np.repeat(sensedf2.loc[sensedf2.index == 0].values,len(refsenses),axis=0),columns=sensedf2.columns)    # copy all columns for current word as scaffolding for referred senses\n",
    "        \n",
    "        for s in sensedf_add.index:\n",
    "            sensedf_add.loc[s, 'pos'] = refsenses.loc[s, 'pos']\n",
    "            sensedf_add.loc[s, 'gloss'] = refsenses.loc[s, 'gloss']\n",
    "            sensedf_add.loc[s, 'mergedSense'] = 1\n",
    "            sensedf_add.loc[s, 'glossNo'] = refsenses.loc[s, 'glossNo']\n",
    "        \n",
    "        #drop duplicate rows\n",
    "        sensedf = pd.concat([sensedf, sensedf_add], axis=0).drop_duplicates(subset = ['pos', 'gloss', 'glossNo']).reset_index()\n",
    "        #glossNo is now a mixture of original glossNo (mergedSense == 0) and reference glossNo (mergedSense == 1)\n",
    "        #set new column with new glossNos\n",
    "        sensedf['mergedGlossNo'] = sensedf.index + 1\n",
    "\n",
    "\n",
    "    #append checked sense df to merged_dictionary\n",
    "    merged_dic = pd.concat([merged_dic, sensedf], axis=0)\n",
    "    #merged_dic.append(sensedf).reset_index()\n",
    "\n",
    "reflog.close()\n",
    "\n",
    "##output dataframe with inflections/ derivations merged in\n",
    "#merged_dic.reset_index().to_csv(folderpath + outFolderName + '/dicmerged_allwords.csv', index = False, encoding='utf-8')\n",
    "merged_dic.reset_index().to_pickle(folderpath + outFolderName + '/dicmerged_' + filetag + parsetag + '.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended to do manual check of referent print-out here for any additional substitutions to merged_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do tally of NOS and NOM (NOTE: NOM only counted when NOM >1)\n",
    "\n",
    "## THE FOLLOWING CAN BE SUBSTITUED FOR OTHER DICTIONARY FILES\n",
    "#dic_df = pd.read_pickle('folderpath + outFolderName + '/dicmerged1811.pkl')\n",
    "dic_df = pd.read_pickle(folderpath + outFolderName + '/dicmerged_' + filetag + parsetag + '.pkl')\n",
    "\n",
    "tally_dic =  pd.pivot_table(data = dic_df, \n",
    "                            index = ['word'],\n",
    "                            values=[\"mergedSense\", \"mergedGlossNo\", \"meaningNo\"],\n",
    "                            aggfunc={\"mergedSense\":np.sum, \"mergedGlossNo\": len, \"meaningNo\": max},\n",
    "                            #if using dictionary without merged senses:\n",
    "                            #values=[\"glossNo\", \"meaningNo\"],\n",
    "                            #aggfunc={\"glossNo\":max, \"meaningNo\": max}\n",
    "                           )\n",
    "tally_dic = tally_dic.rename(columns={\"meaningNo\": \"NOM\",\n",
    "                          \"mergedSense\": \"mergedSenseCount\", \n",
    "                          \"mergedGlossNo\": \"NOS_merged\"})\n",
    "#tally_dic = tally_dic.rename(columns={\"meaningNo\": \"NOM\",\n",
    "#                          \"glossNo\": \"NOS_original\"})\n",
    "\n",
    "tally_dic['word'] = tally_dic.index\n",
    "tally_dic.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#output tallied file -- also includes all the covariate, merging and flagging information\n",
    "merged_dic.merge(tally_dic).to_csv(folderpath + outFolderName + '/dictally_' + filetag + parsetag + '.csv', \n",
    "                                   index = False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
